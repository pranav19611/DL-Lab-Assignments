{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0RTaRGx6fKl"
      },
      "source": [
        "# without attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNYmqNYQVnpp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import random\n",
        "import time\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsyEF7meD780",
        "outputId": "47c08c11-8720-4caf-ca48-10055f7870b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXKEduhFDLHf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "dataset_path = \"/content/space_qa_dataset_large.json\"\n",
        "\n",
        "with open(dataset_path, 'r') as file:\n",
        "    data = json.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeHOrF_wV8ZX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import os\n",
        "# Add this block early in your code\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True, raise_on_error=True)\n",
        "# Download the missing 'punkt_tab' resource\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')  # Check if 'punkt_tab' is already present\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab', quiet=True, raise_on_error=True)  # Download if not present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSw4oWLIWRaL"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "MAX_LEN = 100\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2GchO3vWTeU"
      },
      "outputs": [],
      "source": [
        "# Tokenization and vocabulary building\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.counter = Counter()\n",
        "\n",
        "    def build_vocab(self, sentences, max_size=10000):\n",
        "        for sent in sentences:\n",
        "            tokens = word_tokenize(sent.lower())\n",
        "            self.counter.update(tokens)\n",
        "        for word, _ in self.counter.most_common(max_size):\n",
        "            idx = len(self.word2idx)\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        return [self.word2idx.get(w, 3) for w in word_tokenize(sentence.lower())] + [2]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return ' '.join([self.idx2word[i] for i in indices if i != 0 and i != 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LWm0dzOiWWkg"
      },
      "outputs": [],
      "source": [
        "# Build vocab from context + questions + answers\n",
        "vocab = Vocab()\n",
        "contexts, questions, answers = [], [], []\n",
        "for d in data['data']:\n",
        "    for para in d['paragraphs']:\n",
        "        context = para['context']\n",
        "        for qa in para['qas']:\n",
        "            contexts.append(context)\n",
        "            questions.append(qa['question'])\n",
        "            answers.append(qa['answers'][0]['text'])\n",
        "\n",
        "vocab.build_vocab(contexts + questions + answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8gpyLkkWYsC"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, contexts, questions, answers, vocab):\n",
        "        self.contexts = contexts\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.contexts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = self.vocab.encode(self.contexts[idx] + ' ' + self.questions[idx])[:MAX_LEN]\n",
        "        target_seq = [1] + self.vocab.encode(self.answers[idx])[:MAX_LEN]  # Start with <SOS>\n",
        "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
        "\n",
        "train_data = QADataset(contexts, questions, answers, vocab)\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5cdt01CXoEH"
      },
      "outputs": [],
      "source": [
        "# Pad function\n",
        "def pad_batch(batch):\n",
        "    input_seqs = [item[0] for item in batch]\n",
        "    target_seqs = [item[1] for item in batch]\n",
        "    input_seqs = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True)\n",
        "    target_seqs = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True)\n",
        "    return input_seqs, target_seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TUM_4S5XqZS"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(1)  # (batch, 1)\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlXGAQ7eXsR0"
      },
      "outputs": [],
      "source": [
        "# Seq2Seq Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        trg_len = trg.size(1)\n",
        "        trg_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdOi5Am3XuKI",
        "outputId": "42357d7b-ef49-4073-897f-513e4419f01d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 486.8393, Training Time: 3.14s\n",
            "Epoch 2, Loss: 320.5604, Training Time: 4.09s\n",
            "Epoch 3, Loss: 174.9938, Training Time: 2.96s\n",
            "Epoch 4, Loss: 72.7398, Training Time: 2.96s\n",
            "Epoch 5, Loss: 29.0133, Training Time: 2.72s\n",
            "Epoch 6, Loss: 15.5804, Training Time: 3.26s\n",
            "Epoch 7, Loss: 8.7898, Training Time: 3.55s\n",
            "Epoch 8, Loss: 5.9210, Training Time: 3.56s\n",
            "Epoch 9, Loss: 4.2779, Training Time: 4.04s\n",
            "Epoch 10, Loss: 3.3302, Training Time: 3.43s\n"
          ]
        }
      ],
      "source": [
        "# Initialize\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(len(vocab.word2idx), EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "decoder = Decoder(len(vocab.word2idx), EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop with timing\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    for batch in train_loader:\n",
        "        src, trg = pad_batch(batch)\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    duration = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Training Time: {duration:.2f}s\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"qa_model.pt\")\n",
        "\n",
        "# Inference and evaluation\n",
        "\n",
        "def predict(model, context, question, vocab, max_len=20):\n",
        "    model.eval()\n",
        "    input_seq = vocab.encode(context + ' ' + question)\n",
        "    input_tensor = torch.tensor(input_seq).unsqueeze(0).to(DEVICE)\n",
        "    hidden, cell = model.encoder(input_tensor)\n",
        "    input_token = torch.tensor([1]).to(DEVICE)\n",
        "    result = []\n",
        "    for _ in range(max_len):\n",
        "        output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "        top1 = output.argmax(1)\n",
        "        if top1.item() == 2:\n",
        "            break\n",
        "        result.append(top1.item())\n",
        "        input_token = top1\n",
        "    return vocab.decode(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m6BoQ8ZXzab",
        "outputId": "ddc77631-249a-4f13-8db8-100e0814ace6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Interactive QA ===\n",
            "Enter context (or type 'exit' to quit):\n",
            "planet\n",
            "Enter your question:\n",
            "Which planet has a thick toxic atmosphere?\n",
            "Answer: venus\n",
            "\n",
            "Enter context (or type 'exit' to quit):\n",
            "exit\n"
          ]
        }
      ],
      "source": [
        "# Interactive Q&A loop\n",
        "print(\"\\n=== Interactive QA ===\")\n",
        "while True:\n",
        "    context = input(\"Enter context (or type 'exit' to quit):\\n\")\n",
        "    if context.lower() == 'exit':\n",
        "        break\n",
        "    question = input(\"Enter your question:\\n\")\n",
        "    answer = predict(model, context, question, vocab)\n",
        "    print(f\"Answer: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6FkAbaa4SEd",
        "outputId": "f262679e-c7e2-4352-9962-a855958f72b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPpDjVPG4A65",
        "outputId": "d5fca780-f75d-42cc-bc64-40235d1bf5f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=bbcddaef107882c8c9a1091092b68f39e6e52a40b87e64cf11320cbed26180c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation on 50 QA pairs:\n",
            "Average BLEU:   0.8981\n",
            "Average METEOR: 0.8106\n",
            "Average ROUGE-L:1.0000\n",
            "Avg Inference Time: 0.0370 sec per sample\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score\n",
        "import json\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "import time\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load your knowledge base JSON\n",
        "with open(\"space_qa_dataset_large.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Reconstruct evaluation set\n",
        "eval_set = []\n",
        "for d in data[\"data\"]:\n",
        "    for para in d[\"paragraphs\"]:\n",
        "        context = para[\"context\"]\n",
        "        for qa in para[\"qas\"]:\n",
        "            question = qa[\"question\"]\n",
        "            answer = qa[\"answers\"][0][\"text\"]\n",
        "            eval_set.append((context, question, answer))\n",
        "\n",
        "# Evaluation metrics setup\n",
        "bleu_scores, meteor_scores, rouge_l_scores = [], [], []\n",
        "total_time = 0\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "for context, question, reference in eval_set[:50]:  # Limit to 50 for speed\n",
        "    start = time.time()\n",
        "    prediction = predict(model, context, question, vocab)\n",
        "    total_time += time.time() - start\n",
        "\n",
        "    # Tokenize\n",
        "    ref_tokens = nltk.word_tokenize(reference.lower())\n",
        "    pred_tokens = nltk.word_tokenize(prediction.lower())\n",
        "\n",
        "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "    meteor = meteor_score([word_tokenize(reference.lower())], word_tokenize(prediction.lower()))\n",
        "    rougeL = scorer.score(reference, prediction)['rougeL'].fmeasure\n",
        "\n",
        "    bleu_scores.append(bleu)\n",
        "    meteor_scores.append(meteor)\n",
        "    rouge_l_scores.append(rougeL)\n",
        "\n",
        "# Display scores\n",
        "print(f\"\\nEvaluation on {len(bleu_scores)} QA pairs:\")\n",
        "print(f\"Average BLEU:   {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
        "print(f\"Average METEOR: {sum(meteor_scores)/len(meteor_scores):.4f}\")\n",
        "print(f\"Average ROUGE-L:{sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")\n",
        "print(f\"Avg Inference Time: {total_time/len(bleu_scores):.4f} sec per sample\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxvSPNaSEpB7"
      },
      "source": [
        "# With attention (Uploaded separately on github with run cells and output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga1C-H511WGa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(filename=\"space_qa_dataset_large.json\"):\n",
        "    \"\"\"Loads the dataset from a JSON file.\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data['data']\n",
        "\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocesses the data to extract context, questions, and answers.\n",
        "    Also, cleans the text and creates start/end token around answer.\n",
        "    \"\"\"\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers_text = [] # Store original answer texts\n",
        "    answer_starts = [] # Store start positions of answers\n",
        "\n",
        "    for topic in data:\n",
        "        for paragraph in topic['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answer = qa['answers'][0]  # Assuming one answer per question\n",
        "                answer_text = answer['text']\n",
        "                answer_start = context.find(answer_text)\n",
        "\n",
        "                contexts.append(context)\n",
        "                questions.append(question)\n",
        "                answers_text.append(answer_text)\n",
        "                answer_starts.append(answer_start)\n",
        "\n",
        "    # Cleaning text (can be expanded)\n",
        "    def clean_text(text):\n",
        "        text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "        text = text.lower()\n",
        "        return text\n",
        "\n",
        "    contexts = [clean_text(c) for c in contexts]\n",
        "    questions = [clean_text(q) for q in questions]\n",
        "\n",
        "    # Create \"target\" which are the contexts with start/end tokens around answers\n",
        "    targets = []\n",
        "    for i in range(len(contexts)):\n",
        "        target = contexts[i]\n",
        "        start_idx = answer_starts[i]\n",
        "        end_idx = start_idx + len(answers_text[i])\n",
        "        target = target[:start_idx] + \" <start> \" + answers_text[i] + \" <end> \" + target[end_idx:]\n",
        "        targets.append(clean_text(target)) # Clean the target as well\n",
        "\n",
        "    return contexts, questions, targets\n",
        "\n",
        "def tokenize_and_pad(contexts, questions, targets, max_len=100):\n",
        "    \"\"\"\n",
        "    Tokenizes the contexts, questions, and answers and pads them to a maximum length.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize contexts\n",
        "    tokenizer_context = Tokenizer(oov_token=\"<OOV>\")\n",
        "    tokenizer_context.fit_on_texts(contexts)\n",
        "    context_sequences = tokenizer_context.texts_to_sequences(contexts)\n",
        "    padded_contexts = pad_sequences(context_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Tokenize questions\n",
        "    tokenizer_question = Tokenizer(oov_token=\"<OOV>\")\n",
        "    tokenizer_question.fit_on_texts(questions)\n",
        "    question_sequences = tokenizer_question.texts_to_sequences(questions)\n",
        "    padded_questions = pad_sequences(question_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Tokenize targets\n",
        "    tokenizer_target = Tokenizer(oov_token=\"<OOV>\", filters='') # Important: No filters for <start> and <end>\n",
        "    tokenizer_target.fit_on_texts(targets)\n",
        "    target_sequences = tokenizer_target.texts_to_sequences(targets)\n",
        "    padded_targets = pad_sequences(target_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    return padded_contexts, padded_questions, padded_targets, tokenizer_context, tokenizer_question, tokenizer_target\n",
        "\n",
        "# --- Main execution ---\n",
        "data = load_data()\n",
        "contexts, questions, targets = preprocess_data(data)\n",
        "\n",
        "# You can adjust max_len as needed\n",
        "padded_contexts, padded_questions, padded_targets, tokenizer_context, tokenizer_question, tokenizer_target = tokenize_and_pad(\n",
        "    contexts, questions, targets, max_len=200\n",
        ")\n",
        "\n",
        "# Vocabulary sizes (for embedding layers)\n",
        "context_vocab_size = len(tokenizer_context.word_index) + 1\n",
        "question_vocab_size = len(tokenizer_question.word_index) + 1\n",
        "target_vocab_size = len(tokenizer_target.word_index) + 1\n",
        "\n",
        "# Split data (adjust test_size as needed)\n",
        "context_train, context_test, question_train, question_test, target_train, target_test = train_test_split(\n",
        "    padded_contexts, padded_questions, padded_targets, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data Preprocessing Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOdqhnl11Z1R"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_model(context_vocab_size, question_vocab_size, target_vocab_size, max_len, embedding_dim=64, lstm_units=256):\n",
        "    \"\"\"\n",
        "    Creates the LSTM encoder-decoder model with Bahdanau attention (fixed dimension issue).\n",
        "\n",
        "    Args:\n",
        "        context_vocab_size: Vocabulary size of the context.\n",
        "        question_vocab_size: Vocabulary size of the question.\n",
        "        target_vocab_size: Vocabulary size of the target.\n",
        "        max_len: Maximum sequence length.\n",
        "        embedding_dim: Dimensionality of the embedding layers.\n",
        "        lstm_units: Number of units in the LSTM layers.\n",
        "\n",
        "    Returns:\n",
        "        The compiled Keras model.\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------- Encoder -------------------\n",
        "    # Context input\n",
        "    context_input = Input(shape=(max_len,), name='context_input')\n",
        "    context_embedding = Embedding(context_vocab_size, embedding_dim, name='context_embedding')(context_input)\n",
        "    context_encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name='context_encoder_lstm')(context_embedding)\n",
        "    context_encoder_outputs, context_encoder_h, context_encoder_c = context_encoder_lstm\n",
        "\n",
        "    # Question input\n",
        "    question_input = Input(shape=(max_len,), name='question_input')\n",
        "    question_embedding = Embedding(question_vocab_size, embedding_dim, name='question_embedding')(question_input)\n",
        "    question_encoder_lstm = LSTM(lstm_units, return_state=True, name='question_encoder_lstm')(question_embedding)\n",
        "    question_encoder_outputs_q, question_encoder_h, question_encoder_c = question_encoder_lstm # Separate output for question\n",
        "\n",
        "    # Concatenate the final states of both encoders to initialize the decoder\n",
        "    encoder_final_state_h = Concatenate(axis=-1, name='encoder_final_state_h')([context_encoder_h, question_encoder_h])\n",
        "    encoder_final_state_c = Concatenate(axis=-1, name='encoder_final_state_c')([context_encoder_c, question_encoder_c])\n",
        "\n",
        "    # Project encoder outputs to match decoder output dimension\n",
        "    encoder_outputs_processed = Dense(lstm_units * 2, activation='relu', name='encoder_output_projection')(context_encoder_outputs)\n",
        "\n",
        "    # ------------------- Decoder -------------------\n",
        "    target_input = Input(shape=(max_len,), name='target_input')\n",
        "    target_embedding = Embedding(target_vocab_size, embedding_dim, name='target_embedding')(target_input)\n",
        "    decoder_lstm = LSTM(lstm_units * 2, return_sequences=True, return_state=True, name='decoder_lstm')(\n",
        "        target_embedding, initial_state=[encoder_final_state_h, encoder_final_state_c]\n",
        "    )\n",
        "    decoder_outputs, _, _ = decoder_lstm\n",
        "\n",
        "    # ------------------- Attention Mechanism (Bahdanau) -------------------\n",
        "    attention = Attention(name='attention')([decoder_outputs, encoder_outputs_processed])  # decoder_outputs (query), processed encoder_outputs (value)\n",
        "    context_vector = Concatenate(axis=-1, name='context_vector')([decoder_outputs, attention])\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(target_vocab_size, activation='softmax', name='output')(context_vector)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[context_input, question_input, target_input], outputs=output) # Only output prediction here\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Re-create and Print Model Summary ---\n",
        "model = create_model(\n",
        "    context_vocab_size, question_vocab_size, target_vocab_size, max_len=padded_contexts.shape[1],\n",
        "    embedding_dim=128, lstm_units=256 # Using the same units\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "print(\"Model Architecture Defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG91-Eop1eIA"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(\n",
        "    [context_train, question_train, target_train],\n",
        "    np.expand_dims(target_train, axis=-1),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=([context_test, question_test, target_test], np.expand_dims(target_test, axis=-1))\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Plotting training history ---\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CBZUetE1gTp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # Import numpy\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def predict(model, context, question, tokenizer_context, tokenizer_question, tokenizer_target, max_len):  # Remove default max_len\n",
        "    \"\"\"\n",
        "    Generates an answer given a context and a question.\n",
        "\n",
        "    Args:\n",
        "        model: The trained Keras model.\n",
        "        context: The input context (string).\n",
        "        question: The input question (string).\n",
        "        tokenizer_context: Tokenizer for the context.\n",
        "        tokenizer_question: Tokenizer for the question.\n",
        "        tokenizer_target: Tokenizer for the target.\n",
        "        max_len: Maximum length of the input sequences (from training).\n",
        "\n",
        "    Returns:\n",
        "        The predicted answer (string).\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess the input\n",
        "    context = clean_text(context)\n",
        "    question = clean_text(question)\n",
        "\n",
        "    context_seq = tokenizer_context.texts_to_sequences([context])\n",
        "    padded_context = pad_sequences(context_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    question_seq = tokenizer_question.texts_to_sequences([question])\n",
        "    padded_question = pad_sequences(question_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Prepare the target input (start with <start> token)\n",
        "    target_seq = tokenizer_target.texts_to_sequences([\"<start>\"])\n",
        "    padded_target = pad_sequences(target_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Prediction loop\n",
        "    for i in range(max_len - 1):  # Change the loop condition\n",
        "        prediction = model.predict([padded_context, padded_question, padded_target], verbose=0)\n",
        "        print(f\"Prediction shape: {prediction.shape}, i: {i}\")  # Debugging: Check prediction shape\n",
        "        print(f\"Raw prediction: {prediction[0, i, :10]}\")  # Debugging: Check raw output\n",
        "        predicted_token_index = np.argmax(prediction[0, i])\n",
        "        predicted_token = tokenizer_target.index_word.get(predicted_token_index, \"<OOV>\")\n",
        "\n",
        "        if predicted_token == \"<end>\":\n",
        "            break\n",
        "\n",
        "        padded_target[0, i + 1] = predicted_token_index\n",
        "\n",
        "    # Decode the predicted sequence\n",
        "    predicted_answer_tokens = [tokenizer_target.index_word.get(idx, \"<OOV>\") for idx in padded_target[0] if idx != 0]\n",
        "    predicted_answer = \" \".join(predicted_answer_tokens)\n",
        "    predicted_answer = predicted_answer.replace(\"<start> \", \"\").replace(\" <end>\", \"\").strip()\n",
        "\n",
        "    return predicted_answer\n",
        "\n",
        "print(\"Prediction Function Defined.\")\n",
        "\n",
        "# --- Example Usage for Visualization ---\n",
        "context_example = \"The solar system has eight planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Mercury is the closest to the Sun.\"\n",
        "question_example = \"What is the closest planet to the Sun?\"\n",
        "target_example = \"mercury\"  # Cleaned version\n",
        "\n",
        "predicted_answer_example = predict(model, context_example, question_example, tokenizer_context, tokenizer_question, tokenizer_target, max_len=padded_contexts.shape[1])\n",
        "\n",
        "print(\"--- Prediction Debugging ---\")\n",
        "print(f\"Context: {context_example}\")\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Predicted Answer: {predicted_answer_example}\")\n",
        "print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVliglqH1kk4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_attention(context, question, predicted_answer, attention_scores, tokenizer_context, tokenizer_target):\n",
        "    \"\"\"\n",
        "    Visualizes the attention weights.\n",
        "\n",
        "    Args:\n",
        "        context: The input context (string).\n",
        "        question: The input question (string).\n",
        "        predicted_answer: The predicted answer (string).\n",
        "        attention_scores: The attention scores from the model.\n",
        "        tokenizer_context: Tokenizer for the context.\n",
        "        tokenizer_target: Tokenizer for the target.\n",
        "    \"\"\"\n",
        "\n",
        "    context_tokens = tokenizer_context.texts_to_sequences([context])[0]\n",
        "    context_words = [tokenizer_context.index_word.get(idx, \"<OOV>\") for idx in context_tokens if idx != 0]\n",
        "\n",
        "    predicted_answer_tokens = tokenizer_target.texts_to_sequences([predicted_answer])[0]\n",
        "    predicted_answer_words = [tokenizer_target.index_word.get(idx, \"<OOV>\") for idx in predicted_answer_tokens if idx != 0]\n",
        "\n",
        "    # Assuming attention_scores has shape (batch, target_seq_len, context_seq_len)\n",
        "    attention_matrix = attention_scores[0]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(attention_matrix, cmap='viridis')\n",
        "    plt.xticks(range(len(context_words)), context_words, rotation='vertical')\n",
        "    plt.yticks(range(len(predicted_answer_words)), predicted_answer_words)\n",
        "    plt.xlabel(\"Context Words\")\n",
        "    plt.ylabel(\"Predicted Answer Words\")\n",
        "    plt.title(\"Attention Visualization\")\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- Example Usage for Visualization ---\n",
        "context_example = \"The solar system has eight planets: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Mercury is the closest to the Sun.\"\n",
        "question_example = \"What is the closest planet to the Sun?\"\n",
        "target_example = \"mercury\" # Cleaned version\n",
        "\n",
        "predicted_answer_example = predict(model, context_example, question_example, tokenizer_context, tokenizer_question, tokenizer_target, max_len=padded_contexts.shape[1])\n",
        "\n",
        "# Get attention scores\n",
        "context_seq = tokenizer_context.texts_to_sequences([context_example])\n",
        "padded_context_example = pad_sequences(context_seq, maxlen=padded_contexts.shape[1], padding='post', truncating='post')\n",
        "\n",
        "question_seq = tokenizer_question.texts_to_sequences([question_example])\n",
        "padded_question_example = pad_sequences(question_seq, maxlen=padded_questions.shape[1], padding='post', truncating='post')\n",
        "\n",
        "target_seq = tokenizer_target.texts_to_sequences([\"<start>\"])\n",
        "padded_target_example = pad_sequences(target_seq, maxlen=padded_targets.shape[1], padding='post', truncating='post')\n",
        "\n",
        "# We need to run the prediction step by step to get attention scores for each output token\n",
        "predicted_answer_tokens_with_start = [\"<start>\"]\n",
        "attention_scores_list = []\n",
        "\n",
        "# Create a new model to get attention scores\n",
        "attention_layer = model.get_layer('attention')\n",
        "attention_model_for_prediction = Model(inputs=model.input, outputs=[model.output, attention_layer.output])\n",
        "\n",
        "for i in range(50): # max_len\n",
        "    target_input_seq = tokenizer_target.texts_to_sequences([predicted_answer_tokens_with_start[-1]])\n",
        "    padded_target_input = pad_sequences(target_input_seq, maxlen=padded_targets.shape[1], padding='post', truncating='post')\n",
        "\n",
        "    output, attention_output = attention_model_for_prediction.predict([padded_context_example, padded_question_example, padded_target_input], verbose=0)\n",
        "    predicted_token_index = np.argmax(output[0, 0])\n",
        "    predicted_token = tokenizer_target.index_word.get(predicted_token_index, \"<OOV>\")\n",
        "\n",
        "    attention_scores_list.append(attention_output[0])  # Store attention scores for this step\n",
        "\n",
        "    predicted_answer_tokens_with_start.append(predicted_token)\n",
        "\n",
        "    if predicted_token == \"<end>\":\n",
        "        break\n",
        "\n",
        "predicted_answer_for_visualization = \" \".join(predicted_answer_tokens_with_start[1:-1]) # Exclude <start> and <end>\n",
        "\n",
        "if attention_scores_list:\n",
        "    visualize_attention(context_example, question_example, predicted_answer_for_visualization, np.array(attention_scores_list), tokenizer_context, tokenizer_target)\n",
        "else:\n",
        "    print(\"Attention scores were not retrieved.\")\n",
        "\n",
        "print(\"Attention Visualization Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KFscs3A1m5w"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "import nltk\n",
        "import nltk.tokenize\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        print(f\"normalize_answer: Input is not a string, converting: {type(s)}\")\n",
        "        s = str(s)\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, ground_truth):\n",
        "    print(f\"compute_exact_match: prediction type = {type(prediction)}, ground_truth type = {type(ground_truth)}\")\n",
        "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def compute_f1(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def compute_rouge(prediction, ground_truth):\n",
        "    rouge = Rouge()\n",
        "    try:\n",
        "        scores = rouge.get_scores(prediction, ground_truth)\n",
        "        return scores[0]['rouge-l']['f']\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "def compute_bleu(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = [normalize_answer(ground_truth).split()]\n",
        "    try:\n",
        "        return sentence_bleu(ground_truth_tokens, prediction_tokens)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "def compute_meteor(prediction, ground_truth):\n",
        "    try:\n",
        "        prediction_tokens = nltk.tokenize.word_tokenize(normalize_answer(prediction))\n",
        "        print(f\"compute_meteor: prediction_tokens type = {type(prediction_tokens)}, ground_truth type = {type(ground_truth)}\")\n",
        "        return single_meteor_score(normalize_answer(ground_truth), prediction_tokens)\n",
        "    except (LookupError, ValueError, TypeError) as e:\n",
        "        print(f\"compute_meteor error: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def find_answer_span(context, target):\n",
        "    \"\"\"Finds the span of the answer within the context, handling errors.\"\"\"\n",
        "    if not isinstance(context, str):\n",
        "        print(f\"find_answer_span: context is not a string, converting: {type(context)}\")\n",
        "        context = str(context)\n",
        "    if not isinstance(target, str):\n",
        "        print(f\"find_answer_span: target is not a string, converting: {type(target)}\")\n",
        "        target = str(target)\n",
        "    context = normalize_answer(context)\n",
        "    target = normalize_answer(target)\n",
        "    start_idx = context.find(target)\n",
        "    if start_idx == -1:\n",
        "        context_tokens = context.split()\n",
        "        target_tokens = target.split()\n",
        "        best_start_idx = -1\n",
        "        max_match = 0\n",
        "        for i in range(len(context_tokens) - len(target_tokens) + 1):\n",
        "            match_count = sum(1 for j in range(len(target_tokens)) if context_tokens[i + j] == target_tokens[j])\n",
        "            if match_count > max_match:\n",
        "                max_match = match_count\n",
        "                best_start_idx = len(\" \".join(context_tokens[:i])) + (1 if i > 0 else 0)\n",
        "\n",
        "        if best_start_idx == -1:\n",
        "            return \"\"\n",
        "        else:\n",
        "            return context[best_start_idx:best_start_idx + len(target)]\n",
        "    else:\n",
        "        return context[start_idx:start_idx + len(target)]\n",
        "\n",
        "def evaluate(model, contexts, questions, answers, tokenizer_context, tokenizer_question, tokenizer_target, max_len):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given data, including ROUGE, BLEU, METEOR, and inference time.\n",
        "    \"\"\"\n",
        "\n",
        "    exact_match_scores = []\n",
        "    f1_scores = []\n",
        "    rouge_scores = []\n",
        "    bleu_scores = []\n",
        "    meteor_scores = []\n",
        "    inference_times = []\n",
        "\n",
        "    num_examples = 5  # Reduce to 5 for even faster debugging\n",
        "\n",
        "    for i in range(min(num_examples, len(questions))):\n",
        "        context = str(contexts[i])\n",
        "        question = str(questions[i])\n",
        "        ground_truth = str(answers[i])\n",
        "\n",
        "        print(f\"evaluate: Processing example {i}, context type = {type(context)}, question type = {type(question)}, ground_truth type = {type(ground_truth)}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        predicted_answer = predict(model, context, question, tokenizer_context, tokenizer_question, tokenizer_target, max_len)\n",
        "        end_time = time.time()\n",
        "        inference_time = end_time - start_time\n",
        "\n",
        "        answer_span = find_answer_span(context, ground_truth)\n",
        "\n",
        "        # *** INSPECTION: Print predicted and ground truth answers ***\n",
        "        print(f\"  --- Example {i} ---\")\n",
        "        print(f\"  Predicted Answer: '{predicted_answer}'\")\n",
        "        print(f\"  Ground Truth Span: '{answer_span}'\")\n",
        "        # *********************************************************\n",
        "\n",
        "        exact_match = compute_exact_match(predicted_answer, answer_span)\n",
        "        f1 = compute_f1(predicted_answer, answer_span)\n",
        "        rouge = compute_rouge(predicted_answer, answer_span)\n",
        "        bleu = compute_bleu(predicted_answer, answer_span)\n",
        "        meteor = compute_meteor(predicted_answer, answer_span)\n",
        "\n",
        "        exact_match_scores.append(exact_match)\n",
        "        f1_scores.append(f1)\n",
        "        rouge_scores.append(rouge)\n",
        "        bleu_scores.append(bleu)\n",
        "        meteor_scores.append(meteor)\n",
        "        inference_times.append(inference_time)\n",
        "\n",
        "    average_exact_match = sum(exact_match_scores) / len(exact_match_scores) if exact_match_scores else 0\n",
        "    average_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
        "    average_rouge = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
        "    average_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "    average_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
        "    average_inference_time = sum(inference_times) / len(inference_times) if inference_times else 0\n",
        "\n",
        "    return {\n",
        "        'exact_match': average_exact_match,\n",
        "        'f1': average_f1,\n",
        "        'rouge': average_rouge,\n",
        "        'bleu': average_bleu,\n",
        "        'meteor': average_meteor,\n",
        "        'average_inference_time': average_inference_time\n",
        "    }\n",
        "\n",
        "# --- Example Usage ---\n",
        "# Assuming you have context_test, question_test, answers_test\n",
        "# which are lists of strings corresponding to your test set.\n",
        "answers_test = []\n",
        "num_examples = 5  # Reduce for consistency\n",
        "for i in range(min(num_examples, len(context_test))):\n",
        "    context = str(context_test[i])\n",
        "    target = str(targets[i])\n",
        "    answers_test.append(target)\n",
        "\n",
        "# Slice the test data to match the number of examples we're evaluating\n",
        "context_test_subset = context_test[:num_examples]\n",
        "question_test_subset = question_test[:num_examples]\n",
        "answers_test_subset = answers_test[:num_examples]\n",
        "\n",
        "evaluation_results = evaluate(model, context_test_subset, question_test_subset, answers_test_subset, tokenizer_context, tokenizer_question, tokenizer_target, max_len=padded_contexts.shape[1])\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Exact Match: {evaluation_results['exact_match']:.4f}\")\n",
        "print(f\"F1 Score: {evaluation_results['f1']:.4f}\")\n",
        "print(f\"ROUGE-L: {evaluation_results['rouge']:.4f}\")\n",
        "print(f\"BLEU: {evaluation_results['bleu']:.4f}\")\n",
        "print(f\"METEOR: {evaluation_results['meteor']:.4f}\")\n",
        "print(f\"Average Inference Time: {evaluation_results['average_inference_time']:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMjccoS21sdw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYqHMd_Qp789"
      },
      "source": [
        "# Self Attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Km02COBGNh",
        "outputId": "34db5e2d-8d11-4875-b749-faab426e4559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0424c8ae1d3d1a51dc122c6a2daa4ffe100883e2f045f75ae777b3808563b0be\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run_space_qa.py\n",
        "\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWhwWIsCBbWs",
        "outputId": "b1430a31-59eb-484e-b306-a723b9169221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenizes a text string using NLTK's word_tokenize.\"\"\"\n",
        "    return nltk.word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js6k_UCkBinB",
        "outputId": "015e37b0-5701-415d-e940-cbf0b9824a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# Hyperparameters\n",
        "# ---------------------------\n",
        "embed_size = 128\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "hidden_size = 256\n",
        "dropout = 0.1\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-5\n",
        "max_len = 50\n",
        "batch_size = 16\n",
        "num_epochs = 10\n",
        "grad_clip = 1.0\n",
        "\n",
        "# ---------------------------\n",
        "# Text Preprocessing\n",
        "# ---------------------------\n",
        "def pad_sequence(seq, max_len):\n",
        "    # If sequence is longer than max_len, truncate it\n",
        "    if len(seq) > max_len:\n",
        "        seq = seq[:max_len]\n",
        "    # Pad if sequence is shorter than max_len\n",
        "    return seq + [0] * (max_len - len(seq))\n",
        "\n",
        "def create_vocab(sentences):\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        vocab.update(tokenize_text(sentence))\n",
        "    vocab = {word: idx+1 for idx, word in enumerate(vocab)}\n",
        "    vocab['<pad>'] = 0\n",
        "    return vocab\n",
        "\n",
        "def text_to_tensor(text, vocab, max_len):\n",
        "    tokenized = tokenize_text(text)\n",
        "    tokenized = [vocab.get(word, vocab['<pad>']) for word in tokenized]\n",
        "    return torch.tensor(pad_sequence(tokenized, max_len))\n",
        "\n",
        "def tensor_to_text(tensor, vocab):\n",
        "    reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "    return ' '.join([reverse_vocab.get(idx, '<unk>') for idx in tensor if idx != 0])"
      ],
      "metadata": {
        "id": "i-U1_Ab5BpFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, context_questions, answers, vocab, max_len=50):\n",
        "        self.context_questions = context_questions\n",
        "        self.answers = answers\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.context_questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context_tensor = text_to_tensor(self.context_questions[idx][0], self.vocab, self.max_len)\n",
        "        question_tensor = text_to_tensor(self.context_questions[idx][1], self.vocab, self.max_len)\n",
        "        answer_tensor = text_to_tensor(self.answers[idx], self.vocab, self.max_len)\n",
        "        return context_tensor, question_tensor, answer_tensor"
      ],
      "metadata": {
        "id": "p7ejGBlwBrvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Basic TransformerQA (Original)\n",
        "# ---------------------------\n",
        "class TransformerQA(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_size, dropout=0.1):\n",
        "        super(TransformerQA, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout),\n",
        "            num_layers\n",
        "        )\n",
        "        self.decoder = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.embedding(src)\n",
        "        tgt_emb = self.embedding(tgt)\n",
        "        memory = self.encoder(src_emb)\n",
        "        output = self.decoder(tgt_emb)\n",
        "        return output"
      ],
      "metadata": {
        "id": "jIsomfDIBw5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Encoder-Decoder Transformer (New)\n",
        "# ---------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, hidden_size, dropout=0.1, max_len=512):\n",
        "        super(TransformerQAModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, max_len)\n",
        "        self.pos_decoder = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_size, dropout)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(embed_size, num_heads, hidden_size, dropout)\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(next(self.parameters()).device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = None\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
        "        src_emb = self.pos_encoder(src_emb)\n",
        "\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.embedding.embedding_dim)\n",
        "        tgt_emb = self.pos_decoder(tgt_emb)\n",
        "\n",
        "        memory = self.encoder(src_emb.transpose(0, 1), src_key_padding_mask=(src == 0))\n",
        "        output = self.decoder(tgt_emb.transpose(0, 1), memory,\n",
        "                              tgt_mask=tgt_mask,\n",
        "                              tgt_key_padding_mask=(tgt == 0),\n",
        "                              memory_key_padding_mask=(src == 0))\n",
        "\n",
        "        return self.output_layer(output.transpose(0, 1))"
      ],
      "metadata": {
        "id": "zlBTEiIgBz2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Train / Eval\n",
        "# ---------------------------\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for context, question, answer in tqdm(train_loader, desc=\"Training\"):\n",
        "        context, question, answer = context.to(device), question.to(device), answer.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(context, question)\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), answer.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, val_loader, device, vocab):\n",
        "    model.eval()\n",
        "    bleu_scores, meteor_scores, rouge_scores = [], [], []\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context, question, answer in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            context, question, answer = context.to(device), question.to(device), answer.to(device)\n",
        "            output = model(context, question)\n",
        "            pred = output.argmax(dim=-1)\n",
        "            pred_text = tensor_to_text(pred[0].cpu().numpy(), vocab)\n",
        "            answer_text = tensor_to_text(answer[0].cpu().numpy(), vocab)\n",
        "            bleu_scores.append(sentence_bleu([answer_text.split()], pred_text.split()))\n",
        "\n",
        "            # Tokenize the answer_text before passing it to meteor_score\n",
        "            meteor_scores.append(meteor_score([answer_text.split()], pred_text.split()))\n",
        "\n",
        "            rouge_scores.append(scorer.score(answer_text, pred_text)['rougeL'].fmeasure)\n",
        "\n",
        "    print(f\"\\nEvaluation Metrics:\")\n",
        "    print(f\"  BLEU Score   : {np.mean(bleu_scores):.4f}\")\n",
        "    print(f\"  METEOR Score : {np.mean(meteor_scores):.4f}\")\n",
        "    print(f\"  ROUGE-L F1   : {np.mean(rouge_scores):.4f}\")"
      ],
      "metadata": {
        "id": "VhSL6fW5B3oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Save / Load\n",
        "# ---------------------------\n",
        "def save_model(model, filename):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    print(f\"Model saved to {filename}\")\n",
        "\n",
        "def load_model(model, filename, device):\n",
        "    model.load_state_dict(torch.load(filename, map_location=device))\n",
        "    model.to(device)\n",
        "    print(f\"Model loaded from {filename}\")"
      ],
      "metadata": {
        "id": "Rx34T_PCB6r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# Interactive QA\n",
        "# ---------------------------\n",
        "def interactive_qa(model, vocab, device, max_len=50):\n",
        "    model.eval()\n",
        "    reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "    print(\"Interactive QA mode. Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        context = input(\"\\nEnter context (or 'exit'): \")\n",
        "        if context.lower() == 'exit': break\n",
        "        question = input(\"Enter question: \")\n",
        "        if question.lower() == 'exit': break\n",
        "\n",
        "        src = text_to_tensor(context, vocab, max_len).unsqueeze(0).to(device)\n",
        "        tgt = text_to_tensor(question, vocab, max_len).unsqueeze(0).to(device)\n",
        "\n",
        "        if isinstance(model, TransformerQAModel):\n",
        "            output_ids = [vocab.get('<pad>', 0)]\n",
        "            for _ in range(max_len):\n",
        "                tgt_input = torch.tensor([output_ids], device=device)\n",
        "                out = model(src, tgt_input)\n",
        "                next_token = out[0, -1].argmax().item()\n",
        "                if next_token == 0:\n",
        "                    break\n",
        "                output_ids.append(next_token)\n",
        "            answer_text = ' '.join([reverse_vocab.get(i, '<unk>') for i in output_ids if i != 0])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                out = model(src, tgt)\n",
        "                pred = out.argmax(dim=-1)[0]\n",
        "                answer_text = tensor_to_text(pred.cpu().numpy(), vocab)\n",
        "\n",
        "        print(f\"\\nGenerated Answer: {answer_text}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with open(\"space_qa_dataset_large.json\", \"r\") as f:\n",
        "        full_data = json.load(f)\n",
        "\n",
        "    context_questions = []\n",
        "    answers = []\n",
        "    for item in full_data[\"data\"]:\n",
        "        for paragraph in item[\"paragraphs\"]:\n",
        "            context = paragraph[\"context\"]\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answer = qa[\"answers\"][0][\"text\"]\n",
        "                context_questions.append((context, question))\n",
        "                answers.append(answer)\n",
        "\n",
        "    vocab = create_vocab([ctx + \" \" + q + \" \" + ans for (ctx, q), ans in zip(context_questions, answers)])\n",
        "    dataset = QADataset(context_questions, answers, vocab, max_len)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "    # === SELECT MODEL HERE ===\n",
        "    model = TransformerQAModel(len(vocab), embed_size, num_heads, num_layers, hidden_size, dropout, max_len).to(device)\n",
        "    # model = TransformerQA(len(vocab), embed_size, num_heads, num_layers, hidden_size, dropout).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        evaluate(model, val_loader, device, vocab)\n",
        "\n",
        "    save_model(model, \"transformer_qa.pth\")\n",
        "\n",
        "    # Optional: interactive_qa(model, vocab, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QfJrAfmB9tl",
        "outputId": "6d0a482a-7255-4e2e-9a64-6e44668f7188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "Training: 100%|| 13/13 [00:02<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.2724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:   2%|         | 1/50 [00:04<03:30,  4.30s/it]/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "Evaluating: 100%|| 50/50 [00:04<00:00, 10.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0045\n",
            "  METEOR Score : 0.1048\n",
            "  ROUGE-L F1   : 0.0617\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  5.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 93.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0146\n",
            "  METEOR Score : 0.1555\n",
            "  ROUGE-L F1   : 0.0872\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 96.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0192\n",
            "  METEOR Score : 0.1698\n",
            "  ROUGE-L F1   : 0.1442\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  4.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 61.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0204\n",
            "  METEOR Score : 0.1841\n",
            "  ROUGE-L F1   : 0.0818\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  4.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 96.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0176\n",
            "  METEOR Score : 0.1770\n",
            "  ROUGE-L F1   : 0.0775\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  5.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 92.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0184\n",
            "  METEOR Score : 0.1714\n",
            "  ROUGE-L F1   : 0.1054\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  5.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 94.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0189\n",
            "  METEOR Score : 0.1727\n",
            "  ROUGE-L F1   : 0.0761\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  4.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 60.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0194\n",
            "  METEOR Score : 0.1875\n",
            "  ROUGE-L F1   : 0.0775\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:02<00:00,  4.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 95.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0191\n",
            "  METEOR Score : 0.1782\n",
            "  ROUGE-L F1   : 0.0784\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 13/13 [00:03<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 50/50 [00:00<00:00, 78.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "  BLEU Score   : 0.0189\n",
            "  METEOR Score : 0.1777\n",
            "  ROUGE-L F1   : 0.0761\n",
            "Model saved to transformer_qa.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}